\documentclass[10pt,twocolumn]{article}

\usepackage{appendix}
\usepackage{graphicx}
\usepackage{listings}
\lstset{
    frame=single,
    breaklines=true,
    numbers=left,
    xleftmargin=5.0ex
}

\title{High-Performance Linpack Optimization}

\author{
    Gurusiddesha Chandrasekhara\\
    Department of Informatics\\
    Technische Universit\"at M\"unchen\\
    guru.rvcs@gmail.com
  	\and
    Madhukar Sollepura Prabhu Shankar\\
    Department of Informatics\\
    Technische Universit\"at M\"unchen\\
    madhutime@gmail.com
    \and
    Madhura 	Kumaraswamy\\
    Department of Informatics\\
    Technische Universit\"at M\"unchen\\
    madhura.kswamy@gmail.com
    \and
    Lukas Grillmayer\\
    Department of Informatics\\
    Technische Universit\"at M\"unchen\\
    lukas.grillmayer@in.tum.de
}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
The purpose of this project is to automate the process of tuning parameters for the Linpack benchmark to obtain the optimal configuration. A Persicope plugin was created to perform this task. The plugin was then tested on consumer grade machines and built on the SuperMUC supercomputer. Results show the best scenario for manipulation of single parameters.
\end{abstract}

% ====================
\section{Introduction}
\label{sec:introduction}
% ====================
%DONE
Supercomputers embody the climax of high-performance computing (HPC) systems. They provide computation power for governments, industries, and researchers. Applications include the general areas of cryptanalysis, simulation, and modeling. With advancing technology the need for more computation power increases constantly in order to solve more complex problems. Demand-driven, the computation capabilities of supercomputers increase rapidly and many institutions compete for the most powerful system. The Top500 project biannually updates a list of the 500 best supercomputers based on the LINPACK benchmark, which measures the system's floating-point rate of execution. At the time of writing this paper, the best supercomputer is "Tianhe-2" at the National Super Computer Center in Guangzhou, China scoring 33.86 petaflops/s \cite{top500}.

LINPACK is a library which solves a dense system of linear equation in double precision on distributed-memory systems. The theoretical peak performance of a system is generally not achieved. For different problem sizes the actual performance is measured and maximized. Along with the problem size, the benchmark result is also influenced by other parameters which are discussed later.

\subsection{Project Description}
%DONE
The High Performance Linpack (HPL) benchmark has a configurable file called HPL.dat, which contains several parameters that may possibly impact the performance of a supercomputer. The goal of this project is to firstly, build HPL on the SuperMUC petascale system, and then, to identify which parameters impact the performance, with special regard to block sizes, type of process mapping (row-major or column-major) and type of swapping algorithm, and to determine how the problem size impacts scalability. Finally, to create a Periscope Tuning Framework (PTF) plugin that automates a new HPL.dat creation using "sed" (stream editor) and tunes the parameters in the SuperMUC system.

\subsection{Outline}
%DONE
In section \ref{sec:tools} tools are introduced which were used in the course of this project. Secondly, we discuss the tuning process and available benchmark parameters in detail before determining an initial configuration for a consumer grade laptop and SuperMUC. The implementation and functioning of the periscope plugin is described in section \ref{sec:implementation}. Next, the benchmark results are presented, highlighting the impact of the automated tuning process. In conclusion we summarize our findings and discuss topics for future work.

% ====================
\section{Tools}
\label{sec:tools}
% ====================


\subsection{Periscope}
%long
Periscope is an online distributed performance analysis tool\cite{tibidabo} developed at the Technische Universit\"at M\"unchen that is used for dynamic tuning, which is a technique to automatically analyze performance issues of parallel MPI applications to evaluate single node performance\cite{tibidabo}. Periscope comprises an analysis agent network, consisting of three agents, namely, the master agent, communication agents and analysis agents, which search for performance problems in one or more experiments, called iterations and use a monitoring interface to conﬁgure the measurements. After the end of the search, the detected problems are communicated to the master agent through the communication agents and the results of the performance problem detection are displayed on a user-interface.


The Periscope Tuning Framework (PTF) was developed under the AutoTune project \cite{apsc} to combine and automate analysis and optimization of aspects such as energy consumption, inter-process communication and load balancing. PTF identiﬁes tuning variants based on expert knowledge, evaluates the variants online and then, produces a report, which describes where and how to improve the code. PTF provides plugins for tuning high-level patterns for GPGPUs, HMPP codelets, MPI runtime, compiler flag selection and the energy consumption \cite{apsc}. 

\subsection{BLAS}
%short
Basic Linear Algebra Subprograms (BLAS) are a collection of routines which provide basic vector and matrix operations. It is efficient and portable, which makes it suitable for usage in high performance calculations. It is required by HPL \cite{blas}.

\subsection{ATLAS}
%short
Automatically Tuned Linear Algebra Software (ATLAS) provides a complete BLAS API for C and Fortran77. For all operations it optimizes performance for a specific machine, allowing it to execute faster than plain BLAS. Since HPL requires BLAS it can be substituted by ATLAS \cite{atlas}.


\subsection{OpenMPI}
%long
%TODO
OpenMPI is an open source implementation of MPI (Message Passing Interface), a message passing library specification commonly used in the HPC community. Maintained by numerous academic, research, and industry partners this platform independent interface became the industry-standard for parallel programs in the last decade and has reached version 3.0 in September 2012. As it is designed for portability, efficiency, and flexibility, it runs on virtually any distributed memory, shared memory, and hybrid systems. Among others, the MPI interface has been implemented for C, C++, Fortran, and Python.


\subsection{High-Performance Linpack}
LINPACK addresses scalability in distributed memory systems by providing additional testing rules and environment through the Highly-Parallel LINPACK (HPL) NxN benchmark, which is used for measuring supercomputer performance in the Top500 list. HPL is a portable implementation of the HPL NxN benchmark, and generates and solves a random dense linear system of equations using Gaussian Elimination with partial pivoting and double precision floating point arithmetic. The HPL package is written in C and requires the implementation of either the BLAS library, which can be automatically generated by the ATLAS tool, or the Vector Signal Image Processing Library (VSIPL). The HPL package provides testing rules to obtain the accuracy of the obtained solution, and timing rules to obtain the computation time to determine the best performance scenario.  

The computational steps performed by HPL to obtain the benchmark rating are :

\begin{lstlisting}
/* Generate and partition matrix data among MPI computing nodes. */
/* ... */

/* All the nodes start at the same time. */
MPI_Barrier(...);

/* Start wall-clock timer. */
HPL_ptimer(...);

/* Solve system of equations. */
HPL_pdgesv(...);

/* Stop wall-clock timer. */
HPL_ptimer(...);

/* Obtain the maximum wall-clock time. */
MPI_Reduce(...);

/* Gather statistics about perofrmance rate (based on the maximum wall-clock time) and accuracy of the solution. */
/* ... */

\end{lstlisting}


\subsection{Installation Script}
%DONE
We created a installation script (see appendix \ref{sec:appendix_installationScript}) which automates the installation process of Periscope and HPL with all dependents. It was created for deployment on a clean installation of Ubuntu 14.04 LTS 64bit Desktop using VirtualBox. We suggest a hard disk size of 16GB or larger. The installation process also depends on the system properties, the installation script can serve as useful guideline.


% ====================
\section{Tuning}
\label{sec:tuning}
% ====================

\subsection{Scalabilty}
Scalability is an important aspect that must be considered while running problems on thousands of nodes. Strong scaling is the process of fixing a workload or the problem size and increasing the number of processors so that a program scales linearly when the speedup increases with the number of processing elements used. With respect to the total energy consumption, embarrassingly parallel codes achieve better energy efficiency with increasing system size[1]. Weak scaling is the process of increasing both the workload or the problem size and the number of processors so that the program scales linearly when the run-time stays constant while increasing the workload or problem size proportionally to the number of processors. HPL is a good weakly scalable application and gets more efficient with increasing memory \cite{tibidabo}.

\subsection{Parameters}
As discussed before HPL needs a set of parameters to derive a scenario for benchmarking. This is done via the HPL.dat file, which lists all available options. If multiple values are provided for individual parameters, HPL will execute all possible combinations in a separate scenario. In the following, parameters relevant for benchmarking are presented \cite{HPLtuning, HPLexplained}:

\begin{itemize}

\item Problem size (N): The problem should be as large as possible, whereas N denotes the size of the coefficient matrix. If the problem size exceeds the available memory, performance will decrease due to swapping. It is suggested to initially use about 80\% of available memory.
\begin{center}
$ N = sqrt ( ( \#nodes * \#memoryPerNode * 0.8 ) / sizeOf(double) )$
\end{center}

\item Block size (NB): It is used for data distribution and computational granularity. Small values are desired as data distribution and load balancing improves. If chosen too small, the message overhead will increase and the available cache hierarchy might not be used properly. A value between 32 and 256 is suggested for modern systems.

\item Process grid size (PxQ): The number of process rows (P) and columns (Q) depends on the number of available nodes and the physical network connection. It defines the number of nodes onto which HPL data should be distributed and executed, the process grid. Q must be slightly larger or equal to P. For simple Ethernet networks P should be very small since network scaling is limited. $P*Q$ must be equal to the number of nodes in the system. If surpassed, additional nodes will not be utilized. The suggested range is:
\begin{center}
$P \leq Q \leq 3*P$	
\end{center}

\item Residual threshold: The residuals of the main algorithm will be compared to this value. The threshold is a real number of magnitude 1, whereas for negative values this threshold will not be checked. A suggested value is $16.0$.

\item Recursive factorization method (RFACT): Panel factorization is a matrix-matrix operation, which divides a large matrix into several submatrices. This is also known as LU decomposition. The left-looking and right-looking variants of this algorithm comply to the Doolittle method, dividing the matrix into lower and upper triangular unit matrices. The Crout variant is similar, but the returned lower matrix is not a unit matrix.

\item Panels in recursion (NDIV): During each iteration of panel factorization the current matrix is divided into NDIV submatrices.

\item Recursion stopping criteria (NBMIN): The panel factorization recursion stops when the matrix is is composed of NBMIN columns or less.

\item Panel factorization method (PFACT): After the factorization recursion stops, the algorithm continues with LU decomposition on a matrix-vector basis using a left-looking, Crout, or right-looking approach defined by RFACT.

\item Lookahead depth (DEPTH): The lookahead depth defines the number of next panels which should be factorized immediately after being updated. For large problem sizes a value of 1 is suggested.

\item Panel broadcasting method (BCAST): After panel factorization is complete, the resulting values are sent to other process columns using messages. Using different approaches leads to changes in execution time and throughput. There are multiple broadcast algorithms available:
\begin{itemize}
\item Increased-ring: Every process has to send a message the next process consequently.
\item Increased-ring (modified): Process 0 sends a message to process 1 and 2 each. 
\item Increasing-2-ring: Processes are split into two groups, whereas process 0 sends a message to the second group and acts as the first element of the first group.
\item Increasing-2-ring (modified): The modification to increasing-2-ring is to additionally apply the increasing-ring algorithm to the first group.
\item Long (bandwidth-reducing): The message is divided into Q equal pieces to be distributed to Q different processes in a synchronized fashion. The submessages are then exchanged between sets of two processes until every process received all parts of the original message.
\item Long (bandwidth-reducing modified): The modification to long (bandwidth reducing) is to additionally apply the increasing-ring algorithm.
\end{itemize}

The best method depends on the problem size and hardware performance. If the platform nodes are expected to outperform the network, "Long (bandwidth reducing)" (Lng) or "Long (bandwith reducing modified)" (LnM) can be recommended.

\item Process mapping(PMAP): This parameter specifies how the MPI processes are mapped to the nodes. It is recommended to use row-major mapping for systems with multi-processor computer nodes.

\item Swap algorithm (SWAP): This parameter specifies the swapping algorithm used during all tests. Available algorithms are based on binary-exchange (bin) and spread-roll (long). For mix, the binary-exchange will be used for a number of columns less than the swapping threshold before switching to spread-roll. The swapping threshold is only used for mix. For bin and long, the equilibrium phase can be enabled / disabled.

\item Storage of panel triangle (L1, U): These parameters specify whether the lower (L1) and upper (U) triangle matrix should be stored in no-transposed or transposed form.

\item Memory alignment: This parameters specifies the memory alignment HPL uses to allocate memory. The value is measured in double.
\end{itemize}

For the purpose of this project following parameters were considered for optimization:
\begin{itemize}
\item Problem size (N)
\item Block size (NB)
\item Process grid size (PxQ)
\item Process mapping (PMAP)
\item Swap algorithm (SWAP)
\item Panel broadcasting method (BCAST)
\end{itemize}

Appendix \ref{sec:appendixHPLdatLaptop} shows the optimal configuration for a Lenovo G510 laptop and appendix \ref{sec:appendixHPLdatSuperMUC} shows the optimal configuration for SuperMUC.



% ====================
\section{Implementation}
\label{sec:implementation}
% ====================

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{ptf_flow}
	\caption{PTF flow}
\end{figure}

%TODO by programmers
\subsection{Periscope Plugin}
Describe Flow Chart, Steps to be adapted etc.

\subsection{Parameter Manipulation - sed}


% ====================
\section{Evaluation}
\label{sec:evaluation}
% ====================
%TODO
Present data, findings, optimal parameters

% ====================
\section{Conclusion}
\label{sec:conclusion}
% ====================
%TODO
Our Framework supports optimization of execution time and flops by changing one flag.


% ====================
\section{Future Work}
\label{sec:futureWork}
% ====================
The developed plugin currently only optimizes one parameter during each run. As a result dependencies between two or more parameters which might influence the overall performance are not considered. Optimizing for multiple parameters will prolong the execution time significantly as each combination hast to be checked.

In the course of this project "flops per second" was used as benchmark metric. Although widely used to classify supercomputers the resulting value can only serve as a baseline for comparison. In general, users of HPC systems desire a minimal execution time and/or low cost. For example "Tianhe-2" has a lower power to performance ratio than "Titan", second place in TOP500, which result in higher usage costs \cite{top500}. Thus, the fastest TOP500 system not necessarily yields the best results for every stakeholder. To optimize by execution time, the presented periscope plugin can be adapted to determine the average execution time over several runs and use it as benchmark metric. Likewise, the energy consumption of a system can be monitored using likwid-powermeter \cite{likwidLRZ,likwidPowermeter}.

Further, HPL solves dense matrix-matrix multiplication problems, but real-life HPC applications also contain differential equations, which need high bandwidth,low latency and irregular data access patterns, which HPL cannot execute efficiently. A new benchmark, called the High Performance Conjugate Gradient (HPCG) can deal better with lower computation-to-data-access ratios\cite{betterLinpack}. Moreover, LINPACK doesn't address issues like system latency and memory hierarchy. 

%TODO running on SuperMUC.

\begin{thebibliography}{9}

\bibitem{top500}
  	TOP500 Supercomputer Site,
  	\emph{November 2014}.
  	http://www.top500.org/lists/2014/11/
\bibitem{likwidLRZ}
	Leibnitz Supercomputing Centre,
	\emph{likwid - light weight performance tools}.
	https://www.lrz.de/services/software/programmierung/likwid/
\bibitem{likwidPowermeter} 
	Liwkid,
	\emph{LikwidPowermeter}.
	https://code.google.com/p/likwid/wiki/LikwidPowermeter
\bibitem{blas}
	BLAS,
	\emph{Basic Linear Algebra Subroutines}.
	http://www.netlib.org/blas/
\bibitem{atlas}
	ATLAS,
	\emph{Automatically Tuned Linear Algebra Software}.
	http://math-atlas.sourceforge.net/
\bibitem{HPLtuning}
	HPL,
	\emph{HPL Tuning}.
	http://www.netlib.org/benchmark/hpl/tuning.html
\bibitem{dongarraluszczek}
	Luszczek and A. Petitet,
	\emph{The LINPACK Benchmark: Past, Present and Future},
	2001, University of Tennessee, USA
\bibitem{HPLexplained}
	Netlib,
	\emph{HPL Algorithm}.
	http://www.netlib.org/benchmark/hpl\_oldest/algorithm.html
\bibitem{betterLinpack}
	Jack Dongarra and Michael Michael Heroux,
	\emph{Toward a new metric for ranking high performance computing systems},
	Sandia Report, SAND2013-4744, Volume 312, 2013
\bibitem{tibidabo}
	Tibidabo,
	\emph{Making the case for an ARM-based HPC system},
	Future Generation Computer Systems, Elsevier, 2013
\bibitem{apsc}
	Miceli, Civario, Sikora, César, Gerndt, Haitof, Navarrete, Benkner, Sandrieser, Morin, and Bodin,
	\emph{AutoTune: A Plugin-Driven Approach to the Automatic Tuning of Parallel Applications}, Lecture Notes in Computer Science, p. 328-342, 2013, Springer Berlin Heidelberg, Germany
\end{thebibliography}

\begin{appendices}
\onecolumn
\section{Installation Script}
\label{sec:appendix_installationScript}

\begin{lstlisting}
#!/usr/bin/env bash
cd ~


### PERISCOPE ###

# install dependencies
apt-get install -y bison flex build-essential gfortran libace-dev libxerces-c-dev libboost-all-dev doxygen automake autoconf libtool m4 libswitch-perl git mpich2
# clone repo
git clone http://periscope.in.tum.de/git-releases/PTF-PPE-Lecture.git
# prepare build
cd PTF-PPE-Lecture
./bootstrap
cd .. 
mkdir build 
cd build
# configure
../PTF-PPE-Lecture/configure --prefix=$HOME/install/periscope --enable-developer-mode 
# find out number of cores
NP=`cat /proc/cpuinfo | grep processor | wc -l`
# build framework
make -j $NP
make install
export PATH=$HOME/install/periscope/bin:$PATH
echo "export PATH=$HOME/install/periscope/bin:$PATH" >> ~/.bashrc


### BLAS ###

# For more information see following resources:
# http://linuxtoolkit.blogspot.de/2013/03/running-linpack-hpl-test-on-linux.html 

mkdir -p ~/src/
cd ~/src/
wget http://www.netlib.org/blas/blas.tgz
tar -zxvf blas.tgz
cd BLAS
gfortran -O3 -std=legacy -m64 -fno-second-underscore -fPIC -c *.f
ar r libfblas.a *.o
ranlib libfblas.a
rm -rf *.o
export BLAS=~/src/BLAS/libfblas.a
ln -s libfblas.a libblas.a
mv ~/src/BLAS /usr/local/


### ATLAS ###

# For more information see following resources:
# http://math-atlas.sourceforge.net/atlas_install/node6.html

cd ~
# disbale cpu throtteling
# Note: CPU throtteling might not be neccessary for virtual machines
apt-get install -y gnome-applets
apt-get install -y gnome-applets-data
#UNABLE TO LOCATE PACKAGE# apt-get install -y libpanel-applets2-0
/usr/bin/cpufreq-selector -g performance &

# download latest ATLAS library
wget http://www.netlib.org/lapack/lapack-3.5.0.tgz
wget http://sourceforge.net/projects/math-atlas/files/Stable/3.10.2/atlas3.10.2.tar.bz2
tar -xvf atlas3.10.2.tar.bz2
mv ATLAS ATLAS3.10.x                          # get unique dir name
cd ATLAS3.10.x                                # enter SRCdir
mkdir Linux_C2D64SSE3                         # create BLDdir
cd Linux_C2D64SSE3                            # enter BLDdir

../configure -b 64 --shared --with-netlib-lapack-tarfile=/home/$USER/lapack-3.5.0.tgz
# Note: These next steps might take very long...
make build                                    # tune & build lib
make check                                    # sanity check correct answer
make ptcheck                                  # sanity check parallel
make time                                     # check if lib is fast
make install                                  # copy libs to install dir


### OpenMPI ###

# For more information see following resources:
# http://www.sysads.co.uk/2014/05/install-open-mpi-1-8-ubuntu-14-04-13-10/

cd ~
# download latest package
wget https://www.open-mpi.org/software/ompi/v1.8/downloads/openmpi-1.8.4.tar.gz
apt-get install -y libibnetdisc-dev
tar -xvf openmpi-1.8.4.tar.gz
cd openmpi-1.8.4.tar.gz
./configure --prefix="/home/$USER/.openmpi"
make
make install
export PATH=$HOME/.openmpi/bin:$PATH
echo "PATH=$HOME/.openmpi/bin:$PATH" >> ~/.bashrc
export LD_LIBRARY_PATH=$HOME/.openmpi/lib/:$LD_LIBRARY_PATH
echo "LD_LIBRARY_PATH=$HOME/.openmpi/lib/:$LD_LIBRARY_PATH" >> ~/.bashrc


### HPL ###

cd ~
# download latest package
wget http://www.netlib.org/benchmark/hpl/hpl.tgz
tar -xvf hpl.tgz
cd hpl
# configure
ARCH="Linux_PII_CBLAS"
cp setup/Make.$ARCH .

# change parameters and practice sed
sed -i '/^ARCH*/ c\ ARCH = $(ARCH)' Make.$ARCH
sed -i '/^TOPdir*/ c\ TOPdir = $(HOME)/hpl' Make.$ARCH
sed -i '/^MPdir*/ c\ MPdir = /usr/lib/openmpi' Make.$ARCH
sed -i '/^MPinc*/ c\ MPinc = -I$(MPdir)/include' Make.$ARCH
sed -i '/^MPlib*/ c\ MPlib = $(MPdir)/lib/libmpi.so' Make.$ARCH
sed -i '/^LAdir*/ c\ LAdir = /usr/local/atlas/lib' Make.$ARCH
sed -i '/^CC*/ c\ CC = /usr/bin/mpicc' Make.$ARCH
sed -i '/^LINKER*/ c\ LINKER = /usr/bin/mpicc' Make.$ARCH

make arch=$ARCH
cd bin/$ARCH

# run HPL
mpirun -np 4 ./xhpl
\end{lstlisting}

\newpage
\section{HPL.dat for Laptop}
\label{sec:appendixHPLdatLaptop}
%TODO paste HPL.dat here
\begin{lstlisting}
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
6            device out (6=stdout,7=stderr,file)
3            # of problems sizes (N)
108304 346573 368233  # N -- fantastically important; see ahead
1            # Number of block sizes
80 112 96    # Block sizes
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
25           # Ps
26           # Qs
8.0          threshold
1            # of panel fact
0 2 1        PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4 2          NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1 2 0        RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
0 3 1 2 4    BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
0            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
256          swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
0            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
\end{lstlisting}

\end{appendices}

\end{document}